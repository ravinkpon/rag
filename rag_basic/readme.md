## RAG Pipeline Overview

<pre>
Documents  â”€â”€â–¶  Chunks  â”€â”€â–¶  Embeddings  â”€â”€â–¶  Vector Search  â”€â”€â–¶  Reranker
                                                         â”‚
Question   â”€â”€â–¶  Query Embedding  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                     Top-3 Context
                                                         â”‚
                                                      LLM Answer
                                                         â”‚
                                                      Evaluation
</pre>
---
## Project Structure
<pre>
.
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ docs.txt # Knowledge base
â”‚ â””â”€â”€ eval_questions.txt # Evaluation questions
â”œâ”€â”€ rag_eval.py # Main RAG evaluation script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
</pre>
---
## ğŸ“ Question Format

**Questions are labeled to enable automatic evaluation:**

<pre>
[A] What does RAG stand for?
[U] Who is the president of Mars?

[A] â†’ Answer exists in documents
[U] â†’ Answer does NOT exist (model must refuse)
</pre>
---
## Models Used

| Component | Model                                  | Purpose            |
| --------- | -------------------------------------- | ------------------ |
| Embedding | `google/embeddinggemma-300m`           | Semantic retrieval |
| Reranker  | `cross-encoder/ms-marco-MiniLM-L-6-v2` | Precise ranking    |
| LLM       | `gpt-4o-mini`                          | Answer generation  |

---
f

DOC_PATH = "data/docs.txt"
QUESTION_PATH = "data/eval_questions.txt"

docs.txt -> knowledge base
eval_question.txt -> question to test your rag


CHUNK_SIZE = 120
OVERLAP = 30
