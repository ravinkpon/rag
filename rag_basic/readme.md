## RAG Pipeline with Evaluation

<pre>
Documents  â”€â”€â–¶  Chunks  â”€â”€â–¶  Embeddings  â”€â”€â–¶  Vector Search  â”€â”€â–¶  Reranker
                                                         â”‚
Question   â”€â”€â–¶  Query Embedding  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                     Top-3 Context
                                                         â”‚
                                                      LLM Answer
                                                         â”‚
                                                      Evaluation
</pre>
---
## Project Structure

```\
.
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ docs.txt # Knowledge base
â”‚ â””â”€â”€ eval_questions.txt # Evaluation questions
â”œâ”€â”€ rag_eval.py # Main RAG evaluation script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---
## ğŸ“ Question Format

**Questions are labeled to enable automatic evaluation:**

<pre>
[A] What does RAG stand for?
[U] Who is the president of Mars?

[A] â†’ Answer exists in documents
[U] â†’ Answer does NOT exist (model must refuse)
</pre>
---
## Models Used

| Component | Model                                  | Purpose            |
| --------- | -------------------------------------- | ------------------ |
| Embedding | `google/embeddinggemma-300m`           | Semantic retrieval |
| Reranker  | `cross-encoder/ms-marco-MiniLM-L-6-v2` | Precise ranking    |
| LLM       | `gpt-4o-mini`                          | Answer generation  |

---

## âš™ï¸ Installation

```bash
pip install -r requirements.txt
```

### Set OpenAI API Key

**Windows (PowerShell)**
```powershell
setx OPENAI_API_KEY "your_api_key_here"
```

**Linux / macOS**
```bash
export OPENAI_API_KEY="your_api_key_here"
```

---

## â–¶ï¸ Run the Project

```bash
python main.py
```

The script will:
- Embed documents
- Retrieve and rerank context
- Generate answers
- Print evaluation results

---

## ğŸ“Š Evaluation Metrics

### Answerable Questions
- Correct answers
- Failed answers
- Retrieval HIT@K

### Unanswerable Questions
- Correct refusals
- Hallucinations

---

## ğŸ¯ Why This Project Matters

This project demonstrates:
- Proper document chunking
- Two-stage retrieval (bi-encoder + cross-encoder)
- Context-restricted generation
- Hallucination detection
- Practical RAG evaluation logic

---